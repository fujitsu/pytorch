diff --git a/horovod/torch/mpi_ops.py b/horovod/torch/mpi_ops.py
index a553900..d99291b 100644
--- a/horovod/torch/mpi_ops.py
+++ b/horovod/torch/mpi_ops.py
@@ -79,7 +79,10 @@ def _check_function(function_factory, tensor):
 
 
 def _allreduce_function_factory(tensor):
-    return 'horovod_torch_allreduce_async_' + tensor.type().replace('.', '_')
+    if tensor.is_mkldnn:
+        return 'horovod_torch_allreduce_async_' + "torch.FloatTensor".replace('.', '_') # XXX
+    else:
+        return 'horovod_torch_allreduce_async_' + tensor.type().replace('.', '_')
 
 
 def _allreduce_async(tensor, output, name, op, prescale_factor, postscale_factor):
@@ -368,8 +371,10 @@ def allgather(tensor, name=None):
 
 
 def _broadcast_function_factory(tensor):
-    return 'horovod_torch_broadcast_async_' + tensor.type().replace('.', '_')
-
+    if tensor.is_mkldnn:
+        return 'horovod_torch_broadcast_async_' + "torch.FloatTensor".replace('.', '_') # XXX
+    else:
+        return 'horovod_torch_broadcast_async_' + tensor.type().replace('.', '_')
 
 def _broadcast_async(tensor, output, root_rank, name):
     function = _check_function(_broadcast_function_factory, tensor)
diff --git a/horovod/torch/mpi_ops_v2.cc b/horovod/torch/mpi_ops_v2.cc
index 737dc3d..27e4e43 100644
--- a/horovod/torch/mpi_ops_v2.cc
+++ b/horovod/torch/mpi_ops_v2.cc
@@ -58,7 +58,7 @@ void DivideInPlace(::torch::Tensor& tensor, int divisor) {
     return;
   }
 #endif
-  tensor.div_(divisor);
+  tensor.mul_(1./divisor);
 }
 
 int DoAllreduce(::torch::Tensor tensor, ::torch::Tensor output, int divisor,
diff --git a/horovod/torch/optimizer.py b/horovod/torch/optimizer.py
index ec19357..e54800f 100644
--- a/horovod/torch/optimizer.py
+++ b/horovod/torch/optimizer.py
@@ -106,8 +106,13 @@ class _DistributedOptimizer(torch.optim.Optimizer):
                 if p.requires_grad:
                     p.grad = p.data.new(p.size()).zero_()
                     self._requires_update.add(p)
-                    p_tmp = p.expand_as(p)
-                    grad_acc = p_tmp.grad_fn.next_functions[0][0]
+                    if p.is_mkldnn:
+                        p_tmp = p.to_dense().expand_as(p)
+                        grad_acc_tmp = p_tmp.grad_fn.next_functions[0][0]
+                        grad_acc = grad_acc_tmp.next_functions[0][0]
+                    else:
+                        p_tmp = p.expand_as(p)
+                        grad_acc = p_tmp.grad_fn.next_functions[0][0]
                     grad_acc.register_hook(self._make_hook(p))
                     self._grad_accs.append(grad_acc)
 
@@ -161,7 +166,8 @@ class _DistributedOptimizer(torch.optim.Optimizer):
         for p, (handle, ctx) in self._handles.items():
             output = synchronize(handle)
             self._allreduce_delay[p] = self.backward_passes_per_step
-            p.grad.set_(self._compression.decompress(output, ctx))
+            if not p.grad.is_mkldnn:
+                p.grad.set_(self._compression.decompress(output, ctx))
         self._handles.clear()
 
         self._synchronized = True
@@ -270,8 +276,13 @@ class _DistributedAdasumOptimizer(torch.optim.Optimizer):
                 if p.requires_grad:
                     p.grad = p.data.new(p.size()).zero_()
                     self._requires_update.add(p)
-                    p_tmp = p.expand_as(p)
-                    grad_acc = p_tmp.grad_fn.next_functions[0][0]
+                    if p.is_mkldnn:
+                        p_tmp = p.to_dense().expand_as(p)
+                        grad_acc_tmp = p_tmp.grad_fn.next_functions[0][0]
+                        grad_acc = grad_acc_tmp.next_functions[0][0]
+                    else:
+                        p_tmp = p.expand_as(p)
+                        grad_acc = p_tmp.grad_fn.next_functions[0][0]
                     grad_acc.register_hook(self._make_hook(p))
                     self._grad_accs.append(grad_acc)
 
